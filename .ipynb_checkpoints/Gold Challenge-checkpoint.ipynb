{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e1426-af69-4cc8-be08-cb213e6ac04c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d442765-8438-4b93-8004-084c435b2cef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390cb263-2175-4274-a331-1697da0f82cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed9efc-c38a-40d2-82fd-f17cc693009f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fcd1736e-5ff3-4e10-a8dd-b5ed99395ef7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlite3\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71631f9c-8a66-451c-a36d-45d6656890e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data.csv', encoding='latin-1')\n",
    "alay_dict = pd.read_csv('new_kamusalay.csv', encoding='latin-1', header=None)\n",
    "alay_dict = alay_dict.rename(columns={0: 'original', \n",
    "                                      1: 'replacement'})\n",
    "abusive_dict = pd.read_csv('abusive.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a772eda-d2c0-4599-bac0-5cbeca2e5efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__' (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [28/Sep/2023 19:54:14] \"GET /docs/ HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Sep/2023 19:54:16] \"GET /flasgger_static/swagger-ui-bundle.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [28/Sep/2023 19:54:16] \"GET /flasgger_static/swagger-ui-standalone-preset.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [28/Sep/2023 19:54:16] \"GET /flasgger_static/swagger-ui.css HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [28/Sep/2023 19:54:16] \"GET /flasgger_static/lib/jquery.min.js HTTP/1.1\" 304 -\n",
      "127.0.0.1 - - [28/Sep/2023 19:54:17] \"GET /docs.json HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Sep/2023 20:05:39] \"POST /text-processing-file HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Sep/2023 20:10:07] \"GET / HTTP/1.1\" 200 -\n",
      "127.0.0.1 - - [28/Sep/2023 20:10:27] \"POST /text-processing HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import request, Flask, jsonify\n",
    "from flasgger import Swagger, LazyString, LazyJSONEncoder, swag_from\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "app.json_encoder = LazyJSONEncoder\n",
    "swagger_template = dict(\n",
    "info = {\n",
    "    'title': LazyString(lambda: 'API Documentation for Data Processing and Modeling'),\n",
    "    'version': LazyString(lambda: '1.0.0'),\n",
    "    'description': LazyString(lambda: 'Dokumentasi API untuk Data Processing and Modeling')\n",
    "    },\n",
    "    host = LazyString(lambda: request.host)\n",
    ")\n",
    "\n",
    "swagger_config = {\n",
    "    \"headers\": [],\n",
    "    \"specs\": [\n",
    "        {\n",
    "            \"endpoint\": 'docs',\n",
    "            \"route\": '/docs.json'\n",
    "        }\n",
    "    ],\n",
    "    \"static_url_path\": \"/flasgger_static\",\n",
    "    \"swagger_ui\": True,\n",
    "    \"specs_route\": \"/docs/\"\n",
    "}\n",
    "swagger = Swagger(app, template=swagger_template,\n",
    "                 config=swagger_config)\n",
    "\n",
    "# DEFINE ENDPOINTS: BASIC GET\n",
    "@swag_from(\"C:/Users/nde/Downloads/Binar Gold Challenge/docs/hello_world.yml\", methods=['GET'])\n",
    "@app.route('/', methods=['GET'])\n",
    "def hello_world():\n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Menyapa Hello World\",\n",
    "        'data': \"Hello World\"\n",
    "    }\n",
    "    response_data=jsonify(json_response)\n",
    "    return response_data\n",
    "\n",
    "# DEFINE ENDPOINTS: POST FOR TEXT PROCESSING FROM TEXT INPUT\n",
    "@swag_from(\"C:/Users/nde/Downloads/Binar Gold Challenge/docs/text_processing.yml\", methods=['POST'])\n",
    "@app.route('/text-processing', methods=['POST'])\n",
    "def text_processing():\n",
    "    global text, new_list\n",
    "    text = request.form.get('text')\n",
    "    text = re.sub('\\n',' ',text) \n",
    "    text = re.sub('rt',' ',text)\n",
    "    text = re.sub('RT',' ',text)\n",
    "    text = re.sub('user',' ',text)\n",
    "    text = re.sub('USER',' ',text)\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text) \n",
    "    text = re.sub('  +', ' ', text)\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n",
    "    text = ' '.join(['' if word in abusive_dict.ABUSIVE.values else word for word in text.split(' ')])\n",
    "    \n",
    "    alay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\n",
    "    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n",
    "\n",
    "    \n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Teks yang baru saja diinput\",\n",
    "        'data': text\n",
    "    }\n",
    "    \n",
    "    response_data=jsonify(json_response)\n",
    "    return response_data\n",
    "       \n",
    "# DEFINE ENDPOINTS: POST FOR TEXT PROCESSING FROM FILE\n",
    "@swag_from(\"C:/Users/nde/Downloads/Binar Gold Challenge/docs/file_processing.yml\", methods=['POST'])\n",
    "@app.route('/text-processing-file', methods=['POST'])\n",
    "def text_processing_file():\n",
    "    global post_df\n",
    "    \n",
    "    # USING REQUEST TO GET FILE THAT HAS BEEN POSTED FROM API ENDPOINT\n",
    "    file = request.files['file']\n",
    "    \n",
    "    # IMPORT FILE OBJECT INTO PANDAS DATAFRAME (YOU CAN SPECIFY NUMBER OF ROWS IMPORTED USING PARAMETER nrows=(integer value) )\n",
    "    post_df = pd.read_csv(file, encoding='latin-1')\n",
    "    \n",
    "    # SET THE TWEET COLUMN ONLY FOR THE DATAFRAME\n",
    "    post_df = post_df[['Tweet']]\n",
    "    \n",
    "    # DROP DUPLICATED TWEETS\n",
    "    post_df.drop_duplicates(inplace=True)\n",
    "    \n",
    "    # CREATE NEW NUMBER OF CHARACTERS (NO_CHAR) COLUMN THAT CONSISTS OF LENGTH OF TWEET CHARACTERS\n",
    "    post_df['no_char'] = post_df['Tweet'].apply(len)\n",
    "    \n",
    "    # CREATE NEW NUMBER OF WORDS (NO_WORDS) COLUMN THAT CONSISTS OF NUMBER OF WORDS OF EACH TWEET\n",
    "    post_df['no_words'] = post_df['Tweet'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    # CREATE A FUNCTION TO CLEAN DATA FROM ANY NON ALPHA-NUMERIC (AND NON-SPACE) CHARACTERS, AND STRIP IT FROM LEADING/TRAILING SPACES\n",
    "    def tweet_cleansing(x):\n",
    "        tweet = x\n",
    "        cleaned_tweet = re.sub(r'[^a-zA-Z0-9 ]','',tweet).strip()\n",
    "        return cleaned_tweet\n",
    "    \n",
    "    # APPLY THE TWEET_CLEANSING FUNCTION ON TWEET COLUMN, AND CREATE A NEW CLEANED_TWEET COLUMN\n",
    "    post_df['cleaned_tweet'] = post_df['Tweet'].apply(lambda x: tweet_cleansing(x))\n",
    "    \n",
    "    # CREATE NEW NO_CHAR, AND NO_WORDS COLUMNS BASED ON CLEANED_TWEET COLUMN\n",
    "    post_df['no_char_2'] = post_df['cleaned_tweet'].apply(len)\n",
    "    post_df['no_words_2'] = post_df['cleaned_tweet'].apply(lambda x: len(x.split()))\n",
    "    \n",
    "    # CREATE A FUNCTION TO COUNT NUMBER OF ABUSIVE WORDS FOUND IN A CLEANED TWEET\n",
    "    def count_abusive(x):\n",
    "        cleaned_tweet = x\n",
    "        matched_list = []\n",
    "        for i in range(len(abusive_dict)):\n",
    "            for j in x.split():\n",
    "                word = abusive_dict['ABUSIVE'].iloc[i]\n",
    "                if word==j.lower():\n",
    "                    matched_list.append(word)\n",
    "        return len(matched_list)\n",
    "    \n",
    "    # APPLY THE FUNCTION TO COUNT ABUSIVE WORDS, AND CREATE A NEW COLUMN BASED OFF OF IT\n",
    "    post_df['estimated_no_abs_words'] = post_df['cleaned_tweet'].apply(lambda x: count_abusive(x))\n",
    "    \n",
    "    # CONNECT / CREATE NEW DATABASE AND CREATE NEW TABLE CONSISTING LISTED TABLES\n",
    "    conn = sqlite3.connect('database_project.db')\n",
    "     \n",
    "    # DO ITERATIONS TO INSERT DATA (EACH ROW) FROM FINAL DATAFRAME (POST_DF)\n",
    "    for i in range(len(post_df)):\n",
    "        tweet = post_df['Tweet'].iloc[i]\n",
    "        no_char = int(post_df['no_char'].iloc[i])\n",
    "        no_words = int(post_df['no_words'].iloc[i])\n",
    "        cleaned_tweet = post_df['cleaned_tweet'].iloc[i]\n",
    "        no_char_2 = int(post_df['no_char_2'].iloc[i])\n",
    "        no_words_2 = int(post_df['no_words_2'].iloc[i])\n",
    "    \n",
    "        q_insertion = \"insert into post_df (Tweet, no_char, no_words, cleaned_tweet, no_char_2, no_words_2) values (?,?,?,?,?,?)\"\n",
    "        conn.execute(q_insertion,(tweet,no_char,no_words,cleaned_tweet,no_char_2,no_words_2))\n",
    "        conn.commit()\n",
    "        \n",
    "    conn.close()\n",
    "    \n",
    "    # OUTPUT THE RESULT IN JSON FORMAT\n",
    "    json_response = {\n",
    "        'status_code': 200,\n",
    "        'description': \"Teks yang sudah diproses\",\n",
    "        'data': list(post_df['cleaned_tweet'])\n",
    "    }\n",
    "        \n",
    "    response_data=jsonify(json_response)\n",
    "    return response_data\n",
    "    \n",
    "    # VISUALIZE THE NUMBER OF ABUSIVE WORDS USING BARPLOT (COUNTPLOT)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    countplot = sns.countplot(data=post_df, x=\"estimated_no_abs_words\")\n",
    "    for p in countplot.patches:\n",
    "        countplot.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() / 2., p.get_height()),  ha = 'center'\n",
    "                            , va = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
    "\n",
    "    %matplotlib inline\n",
    "    warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "    plt.title('Count of Estimated Number of Abusive Words')\n",
    "    plt.xlabel('Estimated Number of Abusive Words')\n",
    "    plt.savefig('new_countplot.jpeg')\n",
    "    \n",
    "    plt.figure(figsize=(20,4))\n",
    "    boxplot = sns.boxplot(data=post_df, x=\"no_words_2\")\n",
    "\n",
    "    print()\n",
    "    \n",
    "    # VISUALIZE THE NUMBER OF WORDS USING BOXPLOT\n",
    "    %matplotlib inline\n",
    "    warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "    plt.title('Number of Words Boxplot (after tweet cleansing)')\n",
    "    plt.xlabel('')\n",
    "\n",
    "    plt.savefig('new_boxplot.jpeg')\n",
    "    \n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    app.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecae31a-d3ce-4eb8-be4f-5f3383a3806a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "def remove_unnecessary_char(text):\n",
    "    text = re.sub('\\n',' ',text) # Remove every '\\n'\n",
    "    text = re.sub('rt',' ',text) # Remove every retweet symbol\n",
    "    text = re.sub('RT',' ',text)\n",
    "    text = re.sub('user',' ',text) # Remove every username\n",
    "    text = re.sub('USER',' ',text)\n",
    "    text = re.sub('((www\\.[^\\s]+)|(https?://[^\\s]+)|(http?://[^\\s]+))',' ',text) # Remove every URL\n",
    "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
    "    return text\n",
    "    \n",
    "def remove_nonaplhanumeric(text):\n",
    "    text = re.sub('[^0-9a-zA-Z]+', ' ', text) \n",
    "    return text\n",
    "\n",
    "alay_dict_map = dict(zip(alay_dict['original'], alay_dict['replacement']))\n",
    "def normalize_alay(text):\n",
    "    return ' '.join([alay_dict_map[word] if word in alay_dict_map else word for word in text.split(' ')])\n",
    "\n",
    "def remove_abusive(text):\n",
    "    text = ' '.join(['' if word in abusive_dict.ABUSIVE.values else word for word in text.split(' ')])\n",
    "    text = re.sub('  +', ' ', text) # Remove extra spaces\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "print(\"remove_nonaplhanumeric: \", remove_nonaplhanumeric(\"Halooo,,,,, duniaa!!\"))\n",
    "print(\"lowercase: \", lowercase(\"Halooo, duniaa!\"))\n",
    "print(\"remove_unnecessary_char: \", remove_unnecessary_char(\"Hehe\\n\\n RT USER USER apa kabs www.google.com\\n  hehe\"))\n",
    "print(\"normalize_alay: \", normalize_alay(\"aamiin adek abis\"))\n",
    "print(\"remove_abusive: \", remove_abusive(\"anak anjing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b9293ba-b673-4772-bb79-0765b0ad7eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISUALIZE THE NUMBER OF ABUSIVE WORDS USING BARPLOT (COUNTPLOT)\n",
    "    plt.figure(figsize=(10,7))\n",
    "    countplot = sns.countplot(data=post_df, x=\"estimated_no_abs_words\")\n",
    "    for p in countplot.patches:\n",
    "        countplot.annotate(format(p.get_height(), '.0f'), (p.get_x() + p.get_width() / 2., p.get_height()),  ha = 'center'\n",
    "                            , va = 'center', xytext = (0, 10), textcoords = 'offset points')\n",
    "\n",
    "    %matplotlib inline\n",
    "    warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "    plt.title('Count of Estimated Number of Abusive Words')\n",
    "    plt.xlabel('Estimated Number of Abusive Words')\n",
    "    plt.savefig('new_countplot.jpeg')\n",
    "    \n",
    "    plt.figure(figsize=(20,4))\n",
    "    boxplot = sns.boxplot(data=post_df, x=\"no_words_2\")\n",
    "\n",
    "    print()\n",
    "    \n",
    "    # VISUALIZE THE NUMBER OF WORDS USING BOXPLOT\n",
    "    %matplotlib inline\n",
    "    warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "    plt.title('Number of Words Boxplot (after tweet cleansing)')\n",
    "    plt.xlabel('')\n",
    "    plt.savefig('new_boxplot.jpeg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e239fbe6-fe14-430d-a031-b7f4ab55ac9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Untuk mengetahui ada berapa banyak ujaran toxic dan tidak \n",
    "print(\"Toxic shape: \", data[(data['HS'] == 1) | (data['Abusive'] == 1)].shape)\n",
    "print(\"Non-toxic shape: \", data[(data['HS'] == 0) & (data['Abusive'] == 0)].shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "my_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
